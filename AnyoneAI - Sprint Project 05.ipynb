{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "wycYqOwt_0KE"
   },
   "source": [
    "# Sentiment Analysis: Large Movie Review Dataset\n",
    "\n",
    "Hi again! You will be expected to finish this on your own, but you can use the available channels on Discord to ask questions and help others. Please read the entire notebook before starting, this will give you a better idea of what you need to accomplish.\n",
    "\n",
    "This project is related to NLP. As you may already know, the most important and hardest part of an NLP project is pre-processing, which is why we are going to focus on that.\n",
    "\n",
    "### Getting the data\n",
    "\n",
    "To access the data for this project, you only need to execute the code below. This will download three files:\n",
    "\n",
    "- `movies_review_train_aai.csv`: Training dataset you must use to train and find the best hyperparameters on your model.\n",
    "\n",
    "- `movies_review_test_aai.csv`: Test dataset to test your model.\n",
    "\n",
    "Basically a basic sentiment analysis problem, as in this case, consists of a classification problem, where the possible output labels are: `positive` and `negative`. Which indicates, if the review of a movie speaks positively or negatively. In our case it is a binary problem, but one could have many more \"feelings\" tagged and thus allow a more granular analysis.\n",
    "\n",
    "### These are the objectives of the project:\n",
    "\n",
    "* Read data that is not in a traditional format.\n",
    "* Put together a set of preprocessing functions that we can use later on any NLP or related problems.\n",
    "* Vectorize the data in order to apply a machine learning model to it: using BoW or TF-IDF.\n",
    "* BoW and TF-IDF are classic ways to vectorize text, but currently we have some more complex ways with better performance, for this we are going to train our own word embedding and use it as a vectorization source for our data.\n",
    "* Train a sentiment analysis model that allows us to detect positive and negative opinions in movie reviews."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "0oiYxBSs_0KH"
   },
   "source": [
    "---\n",
    "## 1. Get the data\n",
    "\n",
    "**Download the data by executing the code below:**\n",
    "\n",
    "`Notes:` Use the target column as `positive`, that way the positive value will be indicated with a value of `1` and negative with a value of `0`. In this case, a split train/test is not necessary because the original data is already separated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/asado/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/asado/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: en-core-web-sm==3.5.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.5.0/en_core_web_sm-3.5.0-py3-none-any.whl in ./assignment5_venv/lib/python3.8/site-packages (3.5.0)\n",
      "Requirement already satisfied: spacy<3.6.0,>=3.5.0 in ./assignment5_venv/lib/python3.8/site-packages (from en-core-web-sm==3.5.0) (3.5.2)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in ./assignment5_venv/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.7.0)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in ./assignment5_venv/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (6.3.0)\n",
      "Requirement already satisfied: jinja2 in ./assignment5_venv/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.1.2)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in ./assignment5_venv/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.8)\n",
      "Requirement already satisfied: numpy>=1.15.0 in ./assignment5_venv/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.24.2)\n",
      "Requirement already satisfied: packaging>=20.0 in ./assignment5_venv/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (23.1)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in ./assignment5_venv/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.4)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in ./assignment5_venv/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.7)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in ./assignment5_venv/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.65.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in ./assignment5_venv/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.3.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in ./assignment5_venv/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.8)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in ./assignment5_venv/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.1.9)\n",
      "Requirement already satisfied: pathy>=0.10.0 in ./assignment5_venv/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.10.1)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in ./assignment5_venv/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.9)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in ./assignment5_venv/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.10.7)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in ./assignment5_venv/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.4.6)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in ./assignment5_venv/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.1.1)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in ./assignment5_venv/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.12)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in ./assignment5_venv/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.28.2)\n",
      "Requirement already satisfied: setuptools in ./assignment5_venv/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (44.0.0)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in ./assignment5_venv/lib/python3.8/site-packages (from typer<0.8.0,>=0.3.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./assignment5_venv/lib/python3.8/site-packages (from jinja2->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.1.2)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in ./assignment5_venv/lib/python3.8/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.0.4)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in ./assignment5_venv/lib/python3.8/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.7.9)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in ./assignment5_venv/lib/python3.8/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./assignment5_venv/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./assignment5_venv/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./assignment5_venv/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.1.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in ./assignment5_venv/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.26.15)\n",
      "\u001b[38;5;2mâœ” Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "from src import data_utils\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "import nltk\n",
    "\n",
    "sys.path.append(os.path.dirname(os.getcwd()))\n",
    "\n",
    "from src import word2vec\n",
    "from src import evaluation\n",
    "from src import text_normalizer\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "train, test = data_utils.get_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Complete in this cell: Complete the function `split_data()` but not here in\n",
    "# the notebook, do it in the python module called `data_utils.py`.\n",
    "# Then make sure this code runs without errors.\n",
    "X_train, y_train, X_test, y_test = data_utils.split_data(train, test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "M4SOofBj_0KM"
   },
   "source": [
    "---\n",
    "## 2. Normalize the data\n",
    "\n",
    "**Create the following functions but not here in the notebook, do it in the python script called `text_normalizer.py` and import them into the notebook (this way you can build your own NLP preprocessing library). In fact, the structure of the functions is already written, you must complete them with the code that you consider necessary.**\n",
    "\n",
    "- `remove_html_tags(text):` to remove all HTML tags that may be present in text.\n",
    "- `remove_accented_chars(text):` to remove accented characters from text\n",
    "- `expand_contractions(text):` to expand contractions of the type, \"don't\" to \"do not\". The contractions are already defined in the \"contractions.py\" file.\n",
    "- `lemmatize_text(text):` to lemmatize text.\n",
    "- `stem_text(text):` to apply stemming (NLTK's PorterStemmer) on text.\n",
    "- `remove_special_chars(text):` to remove special characters from text.\n",
    "- `remove_special_chars(text, remove_digits=True):` to remove numbers, note that it is the same function to remove special characters with the addition of an argument that enables or disables the removal of numbers.\n",
    "- `remove_stopwords(text, stopwords=stop_words):` to remove stopwords from text.\n",
    "- `remove_extra_new_lines(text):` to remove extra newlines from text.\n",
    "- `remove_extra_whitespace(text):` to remove extra whitespaces from text.\n",
    "\n",
    "If you want to add more features that would be great, for example you could start by removing emojis, using different stemming algorithms, etc. The more functions you have the better, remember that the texts are very varied and the preprocessing depends a lot on the source of our data.\n",
    "\n",
    "To apply each of the functions you created and pre-process the dataset, you must use the `normalize_corpus()` function of the `text_normalizer.py` script. In this method each of the functions you wrote is called, in fact you must enable or disable what you consider necessary (at this point we leave it to your free choice, for example, you can lemmatize or apply stemming or directly not apply any of the two and so on with the rest, but that is your choice), this function simply groups the previous ones for a more simplified use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "H4rI96sr_0KN"
   },
   "outputs": [],
   "source": [
    "stop_words = nltk.corpus.stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "rJtdCt9d_0KN"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/asado/Descargas/Anyone AI/assignment 5/src/text_normalizer.py:38: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  soup = BeautifulSoup(text, \"html.parser\")\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# TODO: Complete all the functions with the `TODO` comment inside the\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39m# module `text_normalizer.py`. Then make sure this code runs without errors.\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39m# You can change the parameters for `normalize_corpus()` if you want.\u001b[39;00m\n\u001b[1;32m      4\u001b[0m norm_train_reviews \u001b[39m=\u001b[39m text_normalizer\u001b[39m.\u001b[39mnormalize_corpus(X_train, stopwords\u001b[39m=\u001b[39mstop_words)\n\u001b[0;32m----> 5\u001b[0m norm_test_reviews \u001b[39m=\u001b[39m text_normalizer\u001b[39m.\u001b[39;49mnormalize_corpus(X_test, stopwords\u001b[39m=\u001b[39;49mstop_words)\n",
      "File \u001b[0;32m~/Descargas/Anyone AI/assignment 5/src/text_normalizer.py:295\u001b[0m, in \u001b[0;36mnormalize_corpus\u001b[0;34m(corpus, html_stripping, contraction_expansion, accented_char_removal, text_lower_case, text_stemming, text_lemmatization, special_char_removal, remove_digits, stopword_removal, stopwords)\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[39m# Expand contractions\u001b[39;00m\n\u001b[1;32m    294\u001b[0m \u001b[39mif\u001b[39;00m contraction_expansion:\n\u001b[0;32m--> 295\u001b[0m     doc \u001b[39m=\u001b[39m expand_contractions(doc)\n\u001b[1;32m    297\u001b[0m \u001b[39m# Lemmatize text\u001b[39;00m\n\u001b[1;32m    298\u001b[0m \u001b[39mif\u001b[39;00m text_lemmatization:\n",
      "File \u001b[0;32m~/Descargas/Anyone AI/assignment 5/src/text_normalizer.py:227\u001b[0m, in \u001b[0;36mexpand_contractions\u001b[0;34m(text, contraction_mapping)\u001b[0m\n\u001b[1;32m    224\u001b[0m     expanded_contraction \u001b[39m=\u001b[39m first_char \u001b[39m+\u001b[39m expanded_contraction[\u001b[39m1\u001b[39m:]\n\u001b[1;32m    225\u001b[0m     \u001b[39mreturn\u001b[39;00m expanded_contraction\n\u001b[0;32m--> 227\u001b[0m expanded_text \u001b[39m=\u001b[39m contractions_pattern\u001b[39m.\u001b[39;49msub(expand_match, text)\n\u001b[1;32m    228\u001b[0m text \u001b[39m=\u001b[39m re\u001b[39m.\u001b[39msub(\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m, expanded_text)\n\u001b[1;32m    230\u001b[0m \u001b[39mreturn\u001b[39;00m text\n",
      "File \u001b[0;32m~/Descargas/Anyone AI/assignment 5/src/text_normalizer.py:216\u001b[0m, in \u001b[0;36mexpand_contractions.<locals>.expand_match\u001b[0;34m(contraction)\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    202\u001b[0m \u001b[39mExpand english contractions on input string.\u001b[39;00m\n\u001b[1;32m    203\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[39m        Output string.\u001b[39;00m\n\u001b[1;32m    210\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    211\u001b[0m contractions_pattern \u001b[39m=\u001b[39m re\u001b[39m.\u001b[39mcompile(\n\u001b[1;32m    212\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m(\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\u001b[39m\"\u001b[39m\u001b[39m|\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(contraction_mapping\u001b[39m.\u001b[39mkeys())),\n\u001b[1;32m    213\u001b[0m     flags\u001b[39m=\u001b[39mre\u001b[39m.\u001b[39mIGNORECASE \u001b[39m|\u001b[39m re\u001b[39m.\u001b[39mDOTALL,\n\u001b[1;32m    214\u001b[0m )\n\u001b[0;32m--> 216\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mexpand_match\u001b[39m(contraction):\n\u001b[1;32m    217\u001b[0m     match \u001b[39m=\u001b[39m contraction\u001b[39m.\u001b[39mgroup(\u001b[39m0\u001b[39m)\n\u001b[1;32m    218\u001b[0m     first_char \u001b[39m=\u001b[39m match[\u001b[39m0\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# TODO: Complete all the functions with the `TODO` comment inside the\n",
    "# module `text_normalizer.py`. Then make sure this code runs without errors.\n",
    "# You can change the parameters for `normalize_corpus()` if you want.\n",
    "norm_train_reviews = text_normalizer.normalize_corpus(X_train, stopwords=stop_words)\n",
    "norm_test_reviews = text_normalizer.normalize_corpus(X_test, stopwords=stop_words)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "uVKvNzkw_0KO"
   },
   "source": [
    "**(\\*) Functions will be checked using unit tests.**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "ilBNdrN1_0KP"
   },
   "source": [
    "---\n",
    "## 3. Feature Engineering\n",
    "\n",
    "You already have the pre-processed data, now you must vectorize them, because remember that the models only understand numbers. At this stage choose whether you want to vectorize with BoW or with TF-IDF. Later we will train our own embedding but for now we go with a more \"classic\" vectorization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "aDAsFB-L_0KP"
   },
   "outputs": [],
   "source": [
    "# TODO Complete in this cell: Use BoW or TF-IDF to vectorize your data.\n",
    "# Remember to call the `fit()` method only on the train dataset!\n",
    "# Assign the features to the variables `train_features` and `test_features`.\n",
    "\n",
    "# Import required libraries\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# Create an object\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Generating output for TF_IDF\n",
    "\n",
    "train_features = vectorizer.fit_transform(norm_train_reviews)\n",
    "test_features = vectorizer.transform(norm_test_reviews)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "pVEP8DA4_0KQ"
   },
   "source": [
    "## 4. Modeling and Performance Evaluation\n",
    "\n",
    "As we said at the beginning, what interests us most in this part is pre-processing. However, we must train a model, so choose a model of your choice (obviously a classification model, given the problem we are facing) and apply everything we learned. Also if you want you can try several models, the more models you use and know better!\n",
    "\n",
    "**In addition to training the model we ask you to show:**\n",
    "\n",
    "- `Precision`\n",
    "- `Recall`\n",
    "- `F1-Score`\n",
    "- `Classification Report`\n",
    "- `Confusion Matrix`\n",
    "\n",
    "**To do this you must complete the `get_performance` function of the `evaluation.py` script.**\n",
    "\n",
    "**Also, you must complete the `plot_roc` function so that it can show:**\n",
    "\n",
    "- `ROC Curve`\n",
    "- `Obtain the ROC-AUC value (later we will do a small minimum performance check with this value)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "aCiCqub0_0KQ"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [1, 25000]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msvm\u001b[39;00m \u001b[39mimport\u001b[39;00m SVC\n\u001b[1;32m      6\u001b[0m baseline_model \u001b[39m=\u001b[39m SVC(random_state\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, C\u001b[39m=\u001b[39m\u001b[39m0.5\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m baseline_model\u001b[39m.\u001b[39;49mfit(train_features, y_train)\n\u001b[1;32m      9\u001b[0m model_predictions \u001b[39m=\u001b[39m baseline_model\u001b[39m.\u001b[39mpredict(test_features)\n",
      "File \u001b[0;32m~/Descargas/Anyone AI/assignment 5/assignment5_venv/lib/python3.8/site-packages/sklearn/svm/_base.py:192\u001b[0m, in \u001b[0;36mBaseLibSVM.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    190\u001b[0m     check_consistent_length(X, y)\n\u001b[1;32m    191\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 192\u001b[0m     X, y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(\n\u001b[1;32m    193\u001b[0m         X,\n\u001b[1;32m    194\u001b[0m         y,\n\u001b[1;32m    195\u001b[0m         dtype\u001b[39m=\u001b[39;49mnp\u001b[39m.\u001b[39;49mfloat64,\n\u001b[1;32m    196\u001b[0m         order\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mC\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    197\u001b[0m         accept_sparse\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcsr\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    198\u001b[0m         accept_large_sparse\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    199\u001b[0m     )\n\u001b[1;32m    201\u001b[0m y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_targets(y)\n\u001b[1;32m    203\u001b[0m sample_weight \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masarray(\n\u001b[1;32m    204\u001b[0m     [] \u001b[39mif\u001b[39;00m sample_weight \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m sample_weight, dtype\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mfloat64\n\u001b[1;32m    205\u001b[0m )\n",
      "File \u001b[0;32m~/Descargas/Anyone AI/assignment 5/assignment5_venv/lib/python3.8/site-packages/sklearn/base.py:584\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    582\u001b[0m         y \u001b[39m=\u001b[39m check_array(y, input_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39my\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_y_params)\n\u001b[1;32m    583\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 584\u001b[0m         X, y \u001b[39m=\u001b[39m check_X_y(X, y, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcheck_params)\n\u001b[1;32m    585\u001b[0m     out \u001b[39m=\u001b[39m X, y\n\u001b[1;32m    587\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m check_params\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mensure_2d\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mTrue\u001b[39;00m):\n",
      "File \u001b[0;32m~/Descargas/Anyone AI/assignment 5/assignment5_venv/lib/python3.8/site-packages/sklearn/utils/validation.py:1124\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m   1106\u001b[0m X \u001b[39m=\u001b[39m check_array(\n\u001b[1;32m   1107\u001b[0m     X,\n\u001b[1;32m   1108\u001b[0m     accept_sparse\u001b[39m=\u001b[39maccept_sparse,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1119\u001b[0m     input_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mX\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1120\u001b[0m )\n\u001b[1;32m   1122\u001b[0m y \u001b[39m=\u001b[39m _check_y(y, multi_output\u001b[39m=\u001b[39mmulti_output, y_numeric\u001b[39m=\u001b[39my_numeric, estimator\u001b[39m=\u001b[39mestimator)\n\u001b[0;32m-> 1124\u001b[0m check_consistent_length(X, y)\n\u001b[1;32m   1126\u001b[0m \u001b[39mreturn\u001b[39;00m X, y\n",
      "File \u001b[0;32m~/Descargas/Anyone AI/assignment 5/assignment5_venv/lib/python3.8/site-packages/sklearn/utils/validation.py:397\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    395\u001b[0m uniques \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39munique(lengths)\n\u001b[1;32m    396\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(uniques) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m--> 397\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    398\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    399\u001b[0m         \u001b[39m%\u001b[39m [\u001b[39mint\u001b[39m(l) \u001b[39mfor\u001b[39;00m l \u001b[39min\u001b[39;00m lengths]\n\u001b[1;32m    400\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [1, 25000]"
     ]
    }
   ],
   "source": [
    "# TODO Complete in this cell: Create and train your own model.\n",
    "# Having the model trained, use it to make predictions on the test dataset.\n",
    "# Assign the predictions to the variable `model_predictions`, it will be used in the\n",
    "# following cell to evaluate the model performance.\n",
    "from sklearn.svm import SVC\n",
    "baseline_model = SVC(random_state=0, C=0.5)\n",
    "\n",
    "baseline_model.fit(train_features, y_train)\n",
    "model_predictions = baseline_model.predict(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ykz1bOKm_0KR",
    "outputId": "b657b8df-33c8-43b5-d97f-18ead07c624d"
   },
   "outputs": [],
   "source": [
    "accuracy, precision, recall, f1_score = evaluation.get_performance(\n",
    "    model_predictions, y_test\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation.plot_roc(baseline_model, y_test, test_features)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Classifying using K-Means\n",
    "\n",
    "Let use tfidf features computed in last section as vector inputs for kmeans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [1, 25000]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mneighbors\u001b[39;00m \u001b[39mimport\u001b[39;00m KNeighborsClassifier\n\u001b[1;32m      6\u001b[0m kmeans \u001b[39m=\u001b[39m KNeighborsClassifier(n_neighbors\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m, weights\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39muniform\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m kmeans\u001b[39m.\u001b[39;49mfit(train_features, y_train)\n\u001b[1;32m      9\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mConverged after \u001b[39m\u001b[39m{\u001b[39;00mkmeans\u001b[39m.\u001b[39mn_iter_\u001b[39m}\u001b[39;00m\u001b[39m iterations\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     10\u001b[0m kmeans_predictions \u001b[39m=\u001b[39m kmeans\u001b[39m.\u001b[39mpredict(test_features)\n",
      "File \u001b[0;32m~/Descargas/Anyone AI/assignment 5/assignment5_venv/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:215\u001b[0m, in \u001b[0;36mKNeighborsClassifier.fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Fit the k-nearest neighbors classifier from the training dataset.\u001b[39;00m\n\u001b[1;32m    197\u001b[0m \n\u001b[1;32m    198\u001b[0m \u001b[39mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[39m    The fitted k-nearest neighbors classifier.\u001b[39;00m\n\u001b[1;32m    212\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    213\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_params()\n\u001b[0;32m--> 215\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit(X, y)\n",
      "File \u001b[0;32m~/Descargas/Anyone AI/assignment 5/assignment5_venv/lib/python3.8/site-packages/sklearn/neighbors/_base.py:454\u001b[0m, in \u001b[0;36mNeighborsBase._fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_tags()[\u001b[39m\"\u001b[39m\u001b[39mrequires_y\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[1;32m    453\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(X, (KDTree, BallTree, NeighborsBase)):\n\u001b[0;32m--> 454\u001b[0m         X, y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(\n\u001b[1;32m    455\u001b[0m             X, y, accept_sparse\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcsr\u001b[39;49m\u001b[39m\"\u001b[39;49m, multi_output\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, order\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mC\u001b[39;49m\u001b[39m\"\u001b[39;49m\n\u001b[1;32m    456\u001b[0m         )\n\u001b[1;32m    458\u001b[0m     \u001b[39mif\u001b[39;00m is_classifier(\u001b[39mself\u001b[39m):\n\u001b[1;32m    459\u001b[0m         \u001b[39m# Classification targets require a specific format\u001b[39;00m\n\u001b[1;32m    460\u001b[0m         \u001b[39mif\u001b[39;00m y\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m \u001b[39mor\u001b[39;00m y\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m \u001b[39mand\u001b[39;00m y\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n",
      "File \u001b[0;32m~/Descargas/Anyone AI/assignment 5/assignment5_venv/lib/python3.8/site-packages/sklearn/base.py:584\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    582\u001b[0m         y \u001b[39m=\u001b[39m check_array(y, input_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39my\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_y_params)\n\u001b[1;32m    583\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 584\u001b[0m         X, y \u001b[39m=\u001b[39m check_X_y(X, y, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcheck_params)\n\u001b[1;32m    585\u001b[0m     out \u001b[39m=\u001b[39m X, y\n\u001b[1;32m    587\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m check_params\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mensure_2d\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mTrue\u001b[39;00m):\n",
      "File \u001b[0;32m~/Descargas/Anyone AI/assignment 5/assignment5_venv/lib/python3.8/site-packages/sklearn/utils/validation.py:1124\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m   1106\u001b[0m X \u001b[39m=\u001b[39m check_array(\n\u001b[1;32m   1107\u001b[0m     X,\n\u001b[1;32m   1108\u001b[0m     accept_sparse\u001b[39m=\u001b[39maccept_sparse,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1119\u001b[0m     input_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mX\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1120\u001b[0m )\n\u001b[1;32m   1122\u001b[0m y \u001b[39m=\u001b[39m _check_y(y, multi_output\u001b[39m=\u001b[39mmulti_output, y_numeric\u001b[39m=\u001b[39my_numeric, estimator\u001b[39m=\u001b[39mestimator)\n\u001b[0;32m-> 1124\u001b[0m check_consistent_length(X, y)\n\u001b[1;32m   1126\u001b[0m \u001b[39mreturn\u001b[39;00m X, y\n",
      "File \u001b[0;32m~/Descargas/Anyone AI/assignment 5/assignment5_venv/lib/python3.8/site-packages/sklearn/utils/validation.py:397\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    395\u001b[0m uniques \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39munique(lengths)\n\u001b[1;32m    396\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(uniques) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m--> 397\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    398\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    399\u001b[0m         \u001b[39m%\u001b[39m [\u001b[39mint\u001b[39m(l) \u001b[39mfor\u001b[39;00m l \u001b[39min\u001b[39;00m lengths]\n\u001b[1;32m    400\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [1, 25000]"
     ]
    }
   ],
   "source": [
    "# TODO Complete in this cell: Train the k-means clustering model using `n_clusters=2`.\n",
    "# Having the model trained, use it to make predictions on the test dataset.\n",
    "# Assign the predictions to the variable `kmeans_predictions`, it will be used in the\n",
    "# following cell to evaluate the model performance.\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "kmeans = KNeighborsClassifier(n_neighbors=2, weights='uniform')\n",
    "\n",
    "kmeans.fit(train_features, y_train)\n",
    "print(f\"Converged after {kmeans.n_iter_} iterations\")\n",
    "kmeans_predictions = kmeans.predict(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy, precision, recall, f1_score = evaluation.get_performance(\n",
    "    [1 - el for el in kmeans_predictions], y_test\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "c4pZ2FDV_0KS"
   },
   "source": [
    "---\n",
    "## 6. Feature Engineering with Custom Word Embedding\n",
    "\n",
    "### Tokenize reviews and train your own Word Embedding\n",
    "\n",
    "You are going to have to train your own word embedding, for this we are going to use the __gensim__ library. The only requirement we ask of you is that the $vector\\_size=100$.\n",
    "\n",
    "[Here](https://radimrehurek.com/gensim/models/word2vec.html) you can read Gensim's Word2Vec documentation so you can train your own embedding, using the review data as a corpus.\n",
    "\n",
    "As a previous step to training your word embedding you must tokenize the corpus, this may take a bit depending on the size of the dataset and the tokenizer we use, if you want you can try the NLTK tokenizer called `ToktokTokenizer`, which turns out to be a little faster (we hope that this recommendation does not bias your work, try and use the ones you want)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nhx6o1pJ_0KS",
    "outputId": "96b56add-1a85-4516-922d-0d4ee0984540"
   },
   "outputs": [],
   "source": [
    "# TODO Complete in this cell: Tokenize your text corpus and use them to train\n",
    "# a Word2Vec model.\n",
    "\n",
    "# TODO: Create and store here the tokenized train and test data.\n",
    "tokenized_train = []\n",
    "tokenized_test = []\n",
    "\n",
    "# TODO: Train your Word2Vec model and assign it to `model_w2v`.\n",
    "# The output model vector size (w2v_vector_size) is set by default to 100,\n",
    "# you can change it if you want.\n",
    "w2v_vector_size = 100\n",
    "model_w2v = ...\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "1yybEN7P_0KS"
   },
   "source": [
    "### Generate averaged word vector features\n",
    "\n",
    "Once the embedding has been trained, we must use it. Remember that embedding will convert each word you pass to it into a vector of a given dimension (in our case $vector\\_size=100$). So in order to obtain a vector for each review, you must average the vectors of all the words that are part of the same review.\n",
    "\n",
    "The function must have the following form:\n",
    "* `vectorizer(corpus, model, num_features=100)`\n",
    "\n",
    "\n",
    "Where:\n",
    "* `corpus:` corresponds to the entire dataset, in this way we obtain an average vector for each review, with a single call to the function.\n",
    "* `model:` is your trained model.\n",
    "* `num_features:` the dimension of the output vector of your embedding (remember that in our case we set this value to 100).\n",
    "\n",
    "To do this you must complete the `vectorize` function of the `word2vec.py` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DN7oCLhd_0KT"
   },
   "outputs": [],
   "source": [
    "# TODO: Make sure you have completed the `vectorizer()` function from\n",
    "# word2vec.py module.\n",
    "# You don't need to change this code, just make it run without errors.\n",
    "w2v_train_features = word2vec.vectorizer(\n",
    "    corpus=tokenized_train, model=model_w2v, num_features=w2v_vector_size\n",
    ")\n",
    "w2v_test_features = word2vec.vectorizer(\n",
    "    corpus=tokenized_test, model=model_w2v, num_features=w2v_vector_size\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "Ekf7XrPd_0KU"
   },
   "source": [
    "### Modeling\n",
    "\n",
    "Finally train a new model, it can be the same one you used before and compare the results you got using BoW/TF-IDF and Word2Vec.\n",
    "\n",
    "In addition to training the model we ask you to show:\n",
    "\n",
    "* `Accuracy`\n",
    "* `Recall`\n",
    "* `F1-Score`\n",
    "* `Classification Report`\n",
    "* `Confusion Matrix`\n",
    "* `ROC Curve`\n",
    "* `Obtain the ROC-AUC value (later we will do a small minimum performance check with this value)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mxYRc-_8_0KU"
   },
   "outputs": [],
   "source": [
    "# TODO Complete in this cell: Train and choose the best model for the task.\n",
    "# Assign this model to the `best_model` variable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aeFGjvLW_0KU",
    "outputId": "742c841d-b6e5-4a95-a631-3fb1e646897a"
   },
   "outputs": [],
   "source": [
    "# TODO: Use the `get_performance()` function from `evaluation.py` module to show\n",
    "# the model performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pes1qmmJ_0KU",
    "outputId": "ca1bc607-507f-4196-d44c-f2843e8e7b01",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO: Use the `plot_roc()` function from `evaluation.py` module to show\n",
    "# the model ROC curve.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "8KZdYlRs_0KV"
   },
   "source": [
    "## 7. Predict data\n",
    "\n",
    "- Take your best model\n",
    "- Take `test data` (i.e. the dataset after doing the preprocessing and feature engineering part)\n",
    "- Run the data through your model and save the predictions on the `positive` column in the `test` DataFrame (yeah that we've loaded at the very beginning of this notebook).\n",
    "- You will have to use that model to fill values in the positive column using the model predictions\n",
    "- Save the modified version of the DataFrame with the name (`dataset/movies_review_predict_aai.csv`) and don't forget to submit it alongside the rest of this sprint project code.\n",
    "\n",
    "Let's say your best model is called `logistic_word2vec`, then your code should be exactly this:\n",
    "\n",
    "```python\n",
    "    from src import config\n",
    "    from pathlib import Path\n",
    "    DATASET_TEST_PREDICT = str(Path(config.DATASET_ROOT_PATH) / \"movies_review_predict_aai.csv\")\n",
    "    test_preds = logistic_word2vec.predict_proba(w2v_test_features)[:, 1]\n",
    "    test[\"positive\"] = test_preds\n",
    "    test.to_csv(DATASET_TEST_PREDICT, index=False)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "gkopeTCj_0KV"
   },
   "source": [
    "---\n",
    "### OPTIONAL:\n",
    "\n",
    "In our case, we train a word embedding from scratch, which is very good at an educational level, but when applying it to a real problem, we need a lot of data (which is not the case with our problem). Therefore, we invite you to investigate and use one of the `pre-trained Word2Vec models`.\n",
    "\n",
    "If you look for the `Pretrained models` section in this [link](https://radimrehurek.com/gensim/models/word2vec.html), you will find information about the models that Gensim owns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Tags",
  "colab": {
   "collapsed_sections": [],
   "name": "Sentiment_Analysis_NLP_Solved.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
